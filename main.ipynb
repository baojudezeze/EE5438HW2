{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import model\n",
        "import utils"
      ],
      "metadata": {
        "id": "Hk1m12uzHMk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_CIFAR10_data(batch_size, num_workers):\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "    data = torch.cat([d[0] for d in DataLoader(train_set)])\n",
        "\n",
        "    # data augmentation\n",
        "    train_transform = transforms.Compose([\n",
        "        torchvision.transforms.RandomCrop(size=(32, 32), padding=4),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3])),\n",
        "        utils.Cutout()\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3]))\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    val_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    return train_set, val_set, train_loader, val_loader, classes\n"
      ],
      "metadata": {
        "id": "3yG9IFTcHSrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train():\n",
        "    for epoch in range(args.epochs):\n",
        "\n",
        "        model.train()\n",
        "        log.train(len_dataset=len(train_loader))\n",
        "\n",
        "        # train loss\n",
        "        for data in train_loader:\n",
        "            batch, labels = data\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "\n",
        "            # the SAM Optimizer, which needs two forward-backward passes to estimate the \"sharpness-aware\" gradient\n",
        "            # step 1\n",
        "            utils.enable_running_stats(model)\n",
        "            outputs = model(batch)\n",
        "            loss = utils.smooth_crossentropy(outputs, labels, smoothing=args.label_smoothing)\n",
        "            loss.mean().backward()\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "\n",
        "            # step 2\n",
        "            utils.disable_running_stats(model)\n",
        "            utils.smooth_crossentropy(model(batch), labels, smoothing=args.label_smoothing).mean().backward()\n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                correct = torch.argmax(outputs.data, 1) == labels\n",
        "                log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
        "                scheduler(epoch)\n",
        "\n",
        "        model.eval()\n",
        "        log.eval(len_dataset=len(val_loader))\n",
        "\n",
        "        # val loss\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                batch, labels = data\n",
        "                batch, labels = batch.to(device), labels.to(device)\n",
        "                outputs = model(batch)\n",
        "                loss = utils.smooth_crossentropy(outputs, labels)\n",
        "                correct = torch.argmax(outputs, 1) == labels\n",
        "                log(model, loss.cpu(), correct.cpu())\n",
        "    log.flush()\n"
      ],
      "metadata": {
        "id": "m3zv8hVmHV2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def complementary_show(show: bool):\n",
        "    if show:\n",
        "        # create confusion matrix\n",
        "        confusion_mat = confusion_matrix(label_vec, pred_vec)\n",
        "        confusion_df = pd.DataFrame(confusion_mat, index=classes, columns=classes)\n",
        "        print(\"Confusion Matrix\")\n",
        "        print(confusion_df)\n",
        "\n",
        "        # create a report to show the f1-score, precision, recall\n",
        "        report = pd.DataFrame.from_dict(classification_report(pred_vec, label_vec, output_dict=True)).T\n",
        "        report['Label'] = [classes[int(x)] if x.isdigit() else \" \" for x in report.index]\n",
        "        report = report[['Label', 'f1-score', 'precision', 'recall', 'support']]\n",
        "        print(report)\n"
      ],
      "metadata": {
        "id": "sUd5MWaGHYKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # hyperparameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f')\n",
        "    parser.add_argument(\"--batch_size\", default=128, type=int)\n",
        "    parser.add_argument(\"--depth\", default=16, type=int)\n",
        "    parser.add_argument(\"--dropout\", default=0, type=float)\n",
        "    parser.add_argument(\"--epochs\", default=300, type=int)\n",
        "    parser.add_argument(\"--label_smoothing\", default=0.1, type=float, help=\"Use 0.1 for label smoothing.\")\n",
        "    parser.add_argument(\"--learning_rate\", default=0.1, type=float)\n",
        "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"SGD Momentum.\")\n",
        "    parser.add_argument(\"--rho\", default=2.0, type=int, help=\"Rho parameter for SAM.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0005, type=float)\n",
        "    parser.add_argument(\"--width_factor\", default=8, type=int, help=\"How many times wider compared to normal ResNet.\")\n",
        "    parser.add_argument(\"--num_workers\", default=2, type=int)\n",
        "    parser.add_argument(\"--comp_show\", default=True, type=bool)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # cudnn settings\n",
        "    utils.initialize(seed=42)\n",
        "\n",
        "    # device and model\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.WideResNet(args.depth, args.width_factor, dropout=args.dropout, in_channels=3, labels=10).to(device)\n",
        "    # model = model.PyramidNet(dataset='cifar10', depth=110, alpha=64, num_classes=10, bottleneck=False)\n",
        "\n",
        "    # get dataset\n",
        "    train_set, val_set, train_loader, val_loader, classes = get_CIFAR10_data(args.batch_size, args.num_workers)\n",
        "\n",
        "    # log\n",
        "    log = utils.Log(log_each=10)\n",
        "\n",
        "    # record parameters\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"Number of trainable parameters:\", num_params)\n",
        "\n",
        "    # optimizer and scheduler\n",
        "    optimizer = utils.SAM(model.parameters(), torch.optim.SGD, rho=args.rho, adaptive=True,\n",
        "                          lr=args.learning_rate,\n",
        "                          momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    scheduler = utils.StepLR(optimizer, args.learning_rate, args.epochs)\n",
        "\n",
        "    # train model\n",
        "    model_train()\n",
        "\n",
        "    # test the Trained Network\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "         ])\n",
        "    pred_vec = []\n",
        "    label_vec = []\n",
        "    correct = 0\n",
        "    test_loss = 0.0\n",
        "    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            batch, labels = data\n",
        "            batch, labels = batch.to(device), labels.to(device)\n",
        "            outputs = model(batch)\n",
        "            test_loss = utils.smooth_crossentropy(outputs, labels)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            pred_vec.extend(predicted.cpu().numpy())\n",
        "            label_vec.extend(labels.cpu().numpy())\n",
        "    pred_vec = np.array(pred_vec)\n",
        "    label_vec = np.array(label_vec)\n",
        "    test_loss = np.array(test_loss.cpu())\n",
        "    print(\"Test Loss: {:.2f}\".format(test_loss[-1]))\n",
        "    print('Test Accuracy on the 10000 test images: %.2f %%' % (100 * correct / len(test_set)))\n",
        "\n",
        "    # best 96.93 % wrn epoch120\n",
        "    # best 97.27 % wrn epoch200\n",
        "    # best 96.26% pyramid epoch200\n",
        "\n",
        "    # compute the Accuracy, F1-Score, Precision, Recall, Support\n",
        "    complementary_show(args.comp_show)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWuxh--mHZxn",
        "outputId": "f7208a1c-e5ce-4611-a8a4-dd3f73ba5d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 98618245.68it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Number of trainable parameters: 17158106\n",
            "┃       epoch  ┃  Train loss  │    accuracy  ┃        l.r.  │     elapsed  ┃ Valid  loss  │    accuracy  ┃\n",
            "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n",
            "┃           0  ┃      1.3184  │     35.44 %  ┃   1.000e-01  │   03:33 min  ┃      1.1034  │     46.75 %  ┃\n",
            "┃           1  ┃      0.9467  │     55.57 %  ┃   1.000e-01  │   03:37 min  ┃      0.8499  │     60.35 %  ┃\n",
            "┃           2  ┃      0.7615  │     65.41 %  ┃   1.000e-01  │   03:36 min  ┃      0.8010  │     64.24 %  ┃\n",
            "┃           3  ┃      0.6449  │     71.25 %  ┃   1.000e-01  │   03:37 min  ┃      0.6848  │     70.26 %  ┃\n",
            "┃           4  ┃      0.5690  │     75.07 %  ┃   1.000e-01  │   03:36 min  ┃      0.5502  │     76.53 %  ┃\n",
            "┃           5  ┃      0.5177  │     77.56 %  ┃   1.000e-01  │   03:36 min  ┃      0.6788  │     69.00 %  ┃\n",
            "┃           6  ┃      0.4828  │     79.18 %  ┃   1.000e-01  │   03:37 min  ┃      0.5587  │     75.43 %  ┃\n",
            "┃           7  ┃      0.4536  │     80.59 %  ┃   1.000e-01  │   03:36 min  ┃      0.5011  │     78.21 %  ┃\n",
            "┃           8  ┃      0.4289  │     81.78 %  ┃   1.000e-01  │   03:36 min  ┃      0.3947  │     83.58 %  ┃\n",
            "┃           9  ┃      0.4109  │     82.58 %  ┃   1.000e-01  │   03:36 min  ┃      0.4178  │     82.09 %  ┃\n",
            "┃          10  ┃      0.3919  │     83.51 %  ┃   1.000e-01  │   03:36 min  ┃      0.4281  │     82.00 %  ┃\n",
            "┃          11  ┃      0.3821  │     83.80 %  ┃   1.000e-01  │   03:36 min  ┃      0.3905  │     83.35 %  ┃\n",
            "┃          12  ┃      0.3752  │     84.28 %  ┃   1.000e-01  │   03:35 min  ┃      0.3959  │     83.07 %  ┃\n",
            "┃          13  ┃      0.3637  │     84.94 %  ┃   1.000e-01  │   03:35 min  ┃      0.3971  │     82.89 %  ┃\n",
            "┃          14  ┃      0.3548  │     85.23 %  ┃   1.000e-01  │   03:35 min  ┃      0.3622  │     84.57 %  ┃\n",
            "┃          15  ┃      0.3461  │     85.62 %  ┃   1.000e-01  │   03:36 min  ┃      0.3573  │     84.82 %  ┃\n",
            "┃          16  ┃      0.3424  │     85.82 %  ┃   1.000e-01  │   03:36 min  ┃      0.3706  │     84.04 %  ┃\n",
            "┃          17  ┃      0.3363  │     86.02 %  ┃   1.000e-01  │   03:36 min  ┃      0.3283  │     86.35 %  ┃\n",
            "┃          18  ┃      0.3272  │     86.64 %  ┃   1.000e-01  │   03:36 min  ┃      0.3525  │     84.91 %  ┃\n",
            "┃          19  ┃      0.3264  │     86.69 %  ┃   1.000e-01  │   03:36 min  ┃      0.3116  │     87.16 %  ┃\n",
            "┃          20  ┃      0.3207  │     86.94 %  ┃   1.000e-01  │   03:36 min  ┃      0.3337  │     85.99 %  ┃\n",
            "┃          21  ┃      0.3154  │     87.10 %  ┃   1.000e-01  │   03:36 min  ┃      0.3073  │     86.65 %  ┃\n",
            "┃          22  ┃      0.3080  │     87.43 %  ┃   1.000e-01  │   03:35 min  ┃      0.3579  │     84.13 %  ┃\n",
            "┃          23  ┃      0.3070  │     87.48 %  ┃   1.000e-01  │   03:35 min  ┃      0.3711  │     83.78 %  ┃\n",
            "┃          24  ┃      0.3039  │     87.62 %  ┃   1.000e-01  │   03:35 min  ┃      0.3807  │     83.71 %  ┃\n",
            "┃          25  ┃      0.3061  │     87.55 %  ┃   1.000e-01  │   03:35 min  ┃      0.2697  │     89.32 %  ┃\n",
            "┃          26  ┃      0.3007  │     87.76 %  ┃   1.000e-01  │   03:35 min  ┃      0.3318  │     86.06 %  ┃\n",
            "┃          27  ┃      0.2939  │     88.07 %  ┃   1.000e-01  │   03:36 min  ┃      0.3492  │     85.35 %  ┃\n",
            "┃          28  ┃      0.2943  │     88.11 %  ┃   1.000e-01  │   03:36 min  ┃      0.2553  │     90.07 %  ┃\n",
            "┃          29  ┃      0.2925  │     88.13 %  ┃   1.000e-01  │   03:36 min  ┃      0.3021  │     87.43 %  ┃\n",
            "┃          30  ┃      0.2838  │     88.60 %  ┃   1.000e-01  │   03:36 min  ┃      0.2801  │     88.62 %  ┃\n",
            "┃          31  ┃      0.2902  │     88.34 %  ┃   1.000e-01  │   03:35 min  ┃      0.2930  │     87.65 %  ┃\n",
            "┃          32  ┃      0.2839  │     88.64 %  ┃   1.000e-01  │   03:35 min  ┃      0.3361  │     85.69 %  ┃\n",
            "┃          33  ┃      0.2798  │     88.82 %  ┃   1.000e-01  │   03:35 min  ┃      0.3500  │     85.70 %  ┃\n",
            "┃          34  ┃      0.2830  │     88.63 %  ┃   1.000e-01  │   03:35 min  ┃      0.3139  │     86.81 %  ┃\n",
            "┃          35  ┃      0.2764  │     88.86 %  ┃   1.000e-01  │   03:35 min  ┃      0.2768  │     88.61 %  ┃\n",
            "┃          36  ┃      0.2800  │     88.60 %  ┃   1.000e-01  │   03:34 min  ┃      0.2706  │     88.64 %  ┃\n",
            "┃          37  ┃      0.2761  │     89.05 %  ┃   1.000e-01  │   03:35 min  ┃      0.2982  │     87.78 %  ┃\n",
            "┃          38  ┃      0.2807  │     88.71 %  ┃   1.000e-01  │   03:35 min  ┃      0.2762  │     88.53 %  ┃\n",
            "┃          39  ┃      0.2736  │     89.04 %  ┃   1.000e-01  │   03:35 min  ┃      0.3065  │     87.22 %  ┃\n",
            "┃          40  ┃      0.2754  │     88.92 %  ┃   1.000e-01  │   03:34 min  ┃      0.2914  │     88.18 %  ┃\n",
            "┃          41  ┃      0.2692  │     89.33 %  ┃   1.000e-01  │   03:35 min  ┃      0.2959  │     88.09 %  ┃\n",
            "┃          42  ┃      0.2673  │     89.49 %  ┃   1.000e-01  │   03:35 min  ┃      0.3604  │     84.43 %  ┃\n",
            "┃          43  ┃      0.2691  │     89.36 %  ┃   1.000e-01  │   03:35 min  ┃      0.2467  │     89.92 %  ┃\n",
            "┃          44  ┃      0.2662  │     89.48 %  ┃   1.000e-01  │   03:36 min  ┃      0.3688  │     84.18 %  ┃\n",
            "┃          45  ┃      0.2684  │     89.34 %  ┃   1.000e-01  │   03:35 min  ┃      0.2642  │     89.21 %  ┃\n",
            "┃          46  ┃      0.2695  │     89.32 %  ┃   1.000e-01  │   03:34 min  ┃      0.2842  │     88.45 %  ┃\n",
            "┃          47  ┃      0.2674  │     89.43 %  ┃   1.000e-01  │   03:34 min  ┃      0.2883  │     88.19 %  ┃\n",
            "┃          48  ┃      0.2649  │     89.50 %  ┃   1.000e-01  │   03:35 min  ┃      0.2835  │     88.49 %  ┃\n",
            "┃          49  ┃      0.2659  │     89.39 %  ┃   1.000e-01  │   03:35 min  ┃      0.2905  │     87.99 %  ┃\n",
            "┃          50  ┃      0.2610  │     89.63 %  ┃   1.000e-01  │   03:34 min  ┃      0.2550  │     89.27 %  ┃\n",
            "┃          51  ┃      0.2624  │     89.66 %  ┃   1.000e-01  │   03:35 min  ┃      0.2631  │     89.39 %  ┃\n",
            "┃          52  ┃      0.2611  │     89.53 %  ┃   1.000e-01  │   03:35 min  ┃      0.2441  │     90.31 %  ┃\n",
            "┃          53  ┃      0.2615  │     89.74 %  ┃   1.000e-01  │   03:35 min  ┃      0.2697  │     88.83 %  ┃\n",
            "┃          54  ┃      0.2615  │     89.63 %  ┃   1.000e-01  │   03:35 min  ┃      0.2607  │     89.51 %  ┃\n",
            "┃          55  ┃      0.2604  │     89.76 %  ┃   1.000e-01  │   03:34 min  ┃      0.2902  │     87.45 %  ┃\n",
            "┃          56  ┃      0.2572  │     89.90 %  ┃   1.000e-01  │   03:35 min  ┃      0.2756  │     88.28 %  ┃\n",
            "┃          57  ┃      0.2613  │     89.66 %  ┃   1.000e-01  │   03:35 min  ┃      0.2843  │     88.28 %  ┃\n",
            "┃          58  ┃      0.2579  │     89.89 %  ┃   1.000e-01  │   03:35 min  ┃      0.2520  │     89.76 %  ┃\n",
            "┃          59  ┃      0.2596  │     89.88 %  ┃   1.000e-01  │   03:35 min  ┃      0.3251  │     86.52 %  ┃\n",
            "┃          60  ┃      0.2550  │     89.83 %  ┃   1.000e-01  │   03:35 min  ┃      0.2537  │     89.91 %  ┃\n",
            "┃          61  ┃      0.2573  │     89.89 %  ┃   1.000e-01  │   03:35 min  ┃      0.2559  │     89.38 %  ┃\n",
            "┃          62  ┃      0.2540  │     90.07 %  ┃   1.000e-01  │   03:35 min  ┃      0.2678  │     89.48 %  ┃\n",
            "┃          63  ┃      0.2614  │     89.71 %  ┃   1.000e-01  │   03:35 min  ┃      0.2688  │     89.09 %  ┃\n",
            "┃          64  ┃      0.2524  │     90.11 %  ┃   1.000e-01  │   03:35 min  ┃      0.2397  │     90.63 %  ┃\n",
            "┃          65  ┃      0.2498  │     90.16 %  ┃   1.000e-01  │   03:35 min  ┃      0.2335  │     91.04 %  ┃\n",
            "┃          66  ┃      0.2529  │     90.01 %  ┃   1.000e-01  │   03:34 min  ┃      0.2902  │     88.11 %  ┃\n",
            "┃          67  ┃      0.2511  │     90.14 %  ┃   1.000e-01  │   03:35 min  ┃      0.2400  │     90.36 %  ┃\n",
            "┃          68  ┃      0.2577  │     89.83 %  ┃   1.000e-01  │   03:35 min  ┃      0.2666  │     88.97 %  ┃\n",
            "┃          69  ┃      0.2551  │     89.75 %  ┃   1.000e-01  │   03:34 min  ┃      0.2898  │     87.88 %  ┃\n",
            "┃          70  ┃      0.2541  │     89.89 %  ┃   1.000e-01  │   03:34 min  ┃      0.2471  │     90.02 %  ┃\n",
            "┃          71  ┃      0.2527  │     90.23 %  ┃   1.000e-01  │   03:34 min  ┃      0.3027  │     87.13 %  ┃\n",
            "┃          72  ┃      0.2471  │     90.26 %  ┃   1.000e-01  │   03:34 min  ┃      0.3055  │     86.90 %  ┃\n",
            "┃          73  ┃      0.2547  │     89.95 %  ┃   1.000e-01  │   03:34 min  ┃      0.2844  │     88.43 %  ┃\n",
            "┃          74  ┃      0.2494  │     90.32 %  ┃   1.000e-01  │   03:35 min  ┃      0.2451  │     90.22 %  ┃\n",
            "┃          75  ┃      0.2496  │     90.15 %  ┃   1.000e-01  │   03:34 min  ┃      0.3186  │     86.89 %  ┃\n",
            "┃          76  ┃      0.2526  │     90.09 %  ┃   1.000e-01  │   03:34 min  ┃      0.3365  │     85.56 %  ┃\n",
            "┃          77  ┃      0.2495  │     90.29 %  ┃   1.000e-01  │   03:35 min  ┃      0.2615  │     88.86 %  ┃\n",
            "┃          78  ┃      0.2492  │     90.35 %  ┃   1.000e-01  │   03:35 min  ┃      0.2931  │     88.20 %  ┃\n",
            "┃          79  ┃      0.2484  │     90.41 %  ┃   1.000e-01  │   03:35 min  ┃      0.2567  │     89.84 %  ┃\n",
            "┃          80  ┃      0.2501  │     90.22 %  ┃   1.000e-01  │   03:35 min  ┃      0.2587  │     89.61 %  ┃\n",
            "┃          81  ┃      0.2429  │     90.51 %  ┃   1.000e-01  │   03:35 min  ┃      0.2744  │     89.03 %  ┃\n",
            "┃          82  ┃      0.2537  │     90.15 %  ┃   1.000e-01  │   03:35 min  ┃      0.2856  │     88.50 %  ┃\n",
            "┃          83  ┃      0.2443  │     90.62 %  ┃   1.000e-01  │   03:36 min  ┃      0.2950  │     87.65 %  ┃\n",
            "┃          84  ┃      0.2418  │     90.71 %  ┃   1.000e-01  │   03:34 min  ┃      0.2451  │     90.44 %  ┃\n",
            "┃          85  ┃      0.2473  │     90.27 %  ┃   1.000e-01  │   03:35 min  ┃      0.2744  │     88.76 %  ┃\n",
            "┃          86  ┃      0.2455  │     90.38 %  ┃   1.000e-01  │   03:36 min  ┃      0.2843  │     88.73 %  ┃\n",
            "┃          87  ┃      0.2459  │     90.37 %  ┃   1.000e-01  │   03:35 min  ┃      0.2828  │     88.36 %  ┃\n",
            "┃          88  ┃      0.2470  │     90.31 %  ┃   1.000e-01  │   03:34 min  ┃      0.2990  │     87.42 %  ┃\n",
            "┃          89  ┃      0.2427  │     90.61 %  ┃   1.000e-01  │   03:34 min  ┃      0.3052  │     86.86 %  ┃\n",
            "┃          90  ┃      0.1551  │     94.46 %  ┃   2.000e-02  │   03:35 min  ┃      0.1360  │     94.66 %  ┃\n",
            "┃          91  ┃      0.1286  │     95.59 %  ┃   2.000e-02  │   03:35 min  ┃      0.1286  │     94.87 %  ┃\n",
            "┃          92  ┃      0.1185  │     96.08 %  ┃   2.000e-02  │   03:36 min  ┃      0.1251  │     94.86 %  ┃\n",
            "┃          93  ┃      0.1125  │     96.28 %  ┃   2.000e-02  │   03:35 min  ┃      0.1214  │     95.14 %  ┃\n",
            "┃          94  ┃      0.1076  │     96.44 %  ┃   2.000e-02  │   03:35 min  ┃      0.1183  │     95.35 %  ┃\n",
            "┃          95  ┃      0.1046  │     96.56 %  ┃   2.000e-02  │   03:35 min  ┃      0.1220  │     95.20 %  ┃\n",
            "┃          96  ┃      0.1012  │     96.79 %  ┃   2.000e-02  │   03:35 min  ┃      0.1194  │     95.40 %  ┃\n",
            "┃          97  ┃      0.1011  │     96.79 %  ┃   2.000e-02  │   03:35 min  ┃      0.1263  │     94.92 %  ┃\n",
            "┃          98  ┃      0.0970  │     96.97 %  ┃   2.000e-02  │   03:36 min  ┃      0.1219  │     95.45 %  ┃\n",
            "┃          99  ┃      0.0973  │     96.92 %  ┃   2.000e-02  │   03:35 min  ┃      0.1295  │     94.91 %  ┃\n",
            "┃         100  ┃      0.0963  │     96.99 %  ┃   2.000e-02  │   03:35 min  ┃      0.1287  │     94.84 %  ┃\n",
            "┃         101  ┃      0.0970  │     96.90 %  ┃   2.000e-02  │   03:35 min  ┃      0.1300  │     94.67 %  ┃\n",
            "┃         102  ┃      0.0968  │     96.96 %  ┃   2.000e-02  │   03:35 min  ┃      0.1375  │     94.04 %  ┃\n",
            "┃         103  ┃      0.0947  │     97.04 %  ┃   2.000e-02  │   03:35 min  ┃      0.1276  │     94.99 %  ┃\n",
            "┃         104  ┃      0.0954  │     96.99 %  ┃   2.000e-02  │   03:35 min  ┃      0.1284  │     94.81 %  ┃\n",
            "┃         105  ┃      0.0941  │     97.16 %  ┃   2.000e-02  │   03:35 min  ┃      0.1293  │     95.10 %  ┃\n",
            "┃         106  ┃      0.0966  │     97.01 %  ┃   2.000e-02  │   03:36 min  ┃      0.1337  │     94.67 %  ┃\n",
            "┃         107  ┃      0.0960  │     97.09 %  ┃   2.000e-02  │   03:36 min  ┃      0.1197  │     95.25 %  ┃\n",
            "┃         108  ┃      0.0966  │     97.04 %  ┃   2.000e-02  │   03:35 min  ┃      0.1283  │     94.82 %  ┃\n",
            "┃         109  ┃      0.0987  │     96.88 %  ┃   2.000e-02  │   03:36 min  ┃      0.1371  │     94.78 %  ┃\n",
            "┃         110  ┃      0.0927  │     97.15 %  ┃   2.000e-02  │   03:35 min  ┃      0.1388  │     94.37 %  ┃\n",
            "┃         111  ┃      0.0954  │     97.09 %  ┃   2.000e-02  │   03:35 min  ┃      0.1357  │     94.57 %  ┃\n",
            "┃         112  ┃      0.0955  │     97.09 %  ┃   2.000e-02  │   03:34 min  ┃      0.1395  │     94.55 %  ┃\n",
            "┃         113  ┃      0.0919  │     97.23 %  ┃   2.000e-02  │   03:35 min  ┃      0.1492  │     93.84 %  ┃\n",
            "┃         114  ┃      0.0920  │     97.21 %  ┃   2.000e-02  │   03:35 min  ┃      0.1411  │     94.55 %  ┃\n",
            "┃         115  ┃      0.0934  │     97.13 %  ┃   2.000e-02  │   03:36 min  ┃      0.1265  │     95.12 %  ┃\n",
            "┃         116  ┃      0.0934  │     97.18 %  ┃   2.000e-02  │   03:35 min  ┃      0.1296  │     95.07 %  ┃\n",
            "┃         117  ┃      0.0933  │     97.19 %  ┃   2.000e-02  │   03:36 min  ┃      0.1307  │     94.80 %  ┃\n",
            "┃         118  ┃      0.0915  │     97.21 %  ┃   2.000e-02  │   03:35 min  ┃      0.1226  │     95.30 %  ┃\n",
            "┃         119  ┃      0.0912  │     97.21 %  ┃   2.000e-02  │   03:35 min  ┃      0.1468  │     94.42 %  ┃\n",
            "┃         120  ┃      0.0929  │     97.17 %  ┃   2.000e-02  │   03:36 min  ┃      0.1340  │     94.78 %  ┃\n",
            "┃         121  ┃      0.0908  │     97.23 %  ┃   2.000e-02  │   03:35 min  ┃      0.1446  │     94.16 %  ┃\n",
            "┃         122  ┃      0.0909  │     97.27 %  ┃   2.000e-02  │   03:35 min  ┃      0.1317  │     94.84 %  ┃\n",
            "┃         123  ┃      0.0882  │     97.44 %  ┃   2.000e-02  │   03:35 min  ┃      0.1340  │     94.61 %  ┃\n",
            "┃         124  ┃      0.0885  │     97.37 %  ┃   2.000e-02  │   03:35 min  ┃      0.1316  │     94.66 %  ┃\n",
            "┃         125  ┃      0.0916  │     97.35 %  ┃   2.000e-02  │   03:35 min  ┃      0.1343  │     94.74 %  ┃\n",
            "┃         126  ┃      0.0886  │     97.29 %  ┃   2.000e-02  │   03:35 min  ┃      0.1302  │     95.03 %  ┃\n",
            "┃         127  ┃      0.0876  │     97.39 %  ┃   2.000e-02  │   03:35 min  ┃      0.1398  │     94.28 %  ┃\n",
            "┃         128  ┃      0.0885  │     97.34 %  ┃   2.000e-02  │   03:35 min  ┃      0.1345  │     94.76 %  ┃\n",
            "┃         129  ┃      0.0876  │     97.47 %  ┃   2.000e-02  │   03:35 min  ┃      0.1385  │     94.33 %  ┃\n",
            "┃         130  ┃      0.0857  │     97.46 %  ┃   2.000e-02  │   03:35 min  ┃      0.1453  │     94.04 %  ┃\n",
            "┃         131  ┃      0.0878  │     97.37 %  ┃   2.000e-02  │   03:35 min  ┃      0.1278  │     94.92 %  ┃\n",
            "┃         132  ┃      0.0858  │     97.53 %  ┃   2.000e-02  │   03:36 min  ┃      0.1332  │     94.81 %  ┃\n",
            "┃         133  ┃      0.0837  │     97.60 %  ┃   2.000e-02  │   03:35 min  ┃      0.1429  │     94.25 %  ┃\n",
            "┃         134  ┃      0.0870  │     97.49 %  ┃   2.000e-02  │   03:35 min  ┃      0.1283  │     95.00 %  ┃\n",
            "┃         135  ┃      0.0849  │     97.52 %  ┃   2.000e-02  │   03:35 min  ┃      0.1290  │     95.07 %  ┃\n",
            "┃         136  ┃      0.0831  │     97.63 %  ┃   2.000e-02  │   03:35 min  ┃      0.1236  │     95.03 %  ┃\n",
            "┃         137  ┃      0.0857  │     97.53 %  ┃   2.000e-02  │   03:35 min  ┃      0.1244  │     95.31 %  ┃\n",
            "┃         138  ┃      0.0865  │     97.37 %  ┃   2.000e-02  │   03:35 min  ┃      0.1593  │     93.60 %  ┃\n",
            "┃         139  ┃      0.0860  │     97.48 %  ┃   2.000e-02  │   03:35 min  ┃      0.1245  │     94.95 %  ┃\n",
            "┃         140  ┃      0.0832  │     97.58 %  ┃   2.000e-02  │   03:35 min  ┃      0.1284  │     94.93 %  ┃\n",
            "┃         141  ┃      0.0835  │     97.62 %  ┃   2.000e-02  │   03:34 min  ┃      0.1210  │     95.39 %  ┃\n",
            "┃         142  ┃      0.0846  │     97.60 %  ┃   2.000e-02  │   03:35 min  ┃      0.1363  │     94.66 %  ┃\n",
            "┃         143  ┃      0.0852  │     97.45 %  ┃   2.000e-02  │   03:35 min  ┃      0.1343  │     94.62 %  ┃\n",
            "┃         144  ┃      0.0844  │     97.54 %  ┃   2.000e-02  │   03:35 min  ┃      0.1327  │     94.63 %  ┃\n",
            "┃         145  ┃      0.0834  │     97.48 %  ┃   2.000e-02  │   03:36 min  ┃      0.1255  │     94.81 %  ┃\n",
            "┃         146  ┃      0.0791  │     97.82 %  ┃   2.000e-02  │   03:36 min  ┃      0.1342  │     94.52 %  ┃\n",
            "┃         147  ┃      0.0814  │     97.64 %  ┃   2.000e-02  │   03:36 min  ┃      0.1276  │     94.99 %  ┃\n",
            "┃         148  ┃      0.0814  │     97.65 %  ┃   2.000e-02  │   03:35 min  ┃      0.1247  │     95.02 %  ┃\n",
            "┃         149  ┃      0.0813  │     97.67 %  ┃   2.000e-02  │   03:36 min  ┃      0.1275  │     94.92 %  ┃\n",
            "┃         150  ┃      0.0813  │     97.59 %  ┃   2.000e-02  │   03:36 min  ┃      0.1198  │     95.13 %  ┃\n",
            "┃         151  ┃      0.0816  │     97.68 %  ┃   2.000e-02  │   03:35 min  ┃      0.1328  │     94.65 %  ┃\n",
            "┃         152  ┃      0.0802  │     97.71 %  ┃   2.000e-02  │   03:35 min  ┃      0.1376  │     94.62 %  ┃\n",
            "┃         153  ┃      0.0799  │     97.73 %  ┃   2.000e-02  │   03:35 min  ┃      0.1322  │     94.49 %  ┃\n",
            "┃         154  ┃      0.0810  │     97.65 %  ┃   2.000e-02  │   03:35 min  ┃      0.1242  │     95.06 %  ┃\n",
            "┃         155  ┃      0.0804  │     97.71 %  ┃   2.000e-02  │   03:36 min  ┃      0.1294  │     94.82 %  ┃\n",
            "┃         156  ┃      0.0804  │     97.72 %  ┃   2.000e-02  │   03:36 min  ┃      0.1459  │     94.33 %  ┃\n",
            "┃         157  ┃      0.0793  │     97.77 %  ┃   2.000e-02  │   03:35 min  ┃      0.1294  │     95.06 %  ┃\n",
            "┃         158  ┃      0.0811  │     97.64 %  ┃   2.000e-02  │   03:35 min  ┃      0.1337  │     94.68 %  ┃\n",
            "┃         159  ┃      0.0795  │     97.77 %  ┃   2.000e-02  │   03:35 min  ┃      0.1331  │     94.76 %  ┃\n",
            "┃         160  ┃      0.0774  │     97.84 %  ┃   2.000e-02  │   03:36 min  ┃      0.1236  │     95.32 %  ┃\n",
            "┃         161  ┃      0.0784  │     97.71 %  ┃   2.000e-02  │   03:36 min  ┃      0.1220  │     95.22 %  ┃\n",
            "┃         162  ┃      0.0793  │     97.74 %  ┃   2.000e-02  │   03:36 min  ┃      0.1350  │     94.67 %  ┃\n",
            "┃         163  ┃      0.0801  │     97.64 %  ┃   2.000e-02  │   03:36 min  ┃      0.1283  │     94.93 %  ┃\n",
            "┃         164  ┃      0.0804  │     97.70 %  ┃   2.000e-02  │   03:35 min  ┃      0.1318  │     94.75 %  ┃\n",
            "┃         165  ┃      0.0767  │     97.84 %  ┃   2.000e-02  │   03:34 min  ┃      0.1436  │     94.41 %  ┃\n",
            "┃         166  ┃      0.0797  │     97.75 %  ┃   2.000e-02  │   03:36 min  ┃      0.1214  │     95.10 %  ┃\n",
            "┃         167  ┃      0.0793  │     97.75 %  ┃   2.000e-02  │   03:36 min  ┃      0.1395  │     94.42 %  ┃\n",
            "┃         168  ┃      0.0793  │     97.78 %  ┃   2.000e-02  │   03:36 min  ┃      0.1208  │     95.21 %  ┃\n",
            "┃         169  ┃      0.0781  │     97.76 %  ┃   2.000e-02  │   03:36 min  ┃      0.1231  │     95.11 %  ┃\n",
            "┃         170  ┃      0.0788  │     97.70 %  ┃   2.000e-02  │   03:35 min  ┃      0.1216  │     95.23 %  ┃\n",
            "┃         171  ┃      0.0756  │     97.95 %  ┃   2.000e-02  │   03:35 min  ┃      0.1274  │     94.87 %  ┃\n",
            "┃         172  ┃      0.0764  │     97.91 %  ┃   2.000e-02  │   03:35 min  ┃      0.1211  │     95.24 %  ┃\n",
            "┃         173  ┃      0.0760  │     97.86 %  ┃   2.000e-02  │   03:36 min  ┃      0.1334  │     94.58 %  ┃\n",
            "┃         174  ┃      0.0775  │     97.77 %  ┃   2.000e-02  │   03:35 min  ┃      0.1186  │     95.39 %  ┃\n",
            "┃         175  ┃      0.0764  │     97.89 %  ┃   2.000e-02  │   03:35 min  ┃      0.1216  │     95.19 %  ┃\n",
            "┃         176  ┃      0.0776  │     97.74 %  ┃   2.000e-02  │   03:35 min  ┃      0.1362  │     94.55 %  ┃\n",
            "┃         177  ┃      0.0782  │     97.79 %  ┃   2.000e-02  │   03:36 min  ┃      0.1261  │     94.99 %  ┃\n",
            "┃         178  ┃      0.0774  │     97.84 %  ┃   2.000e-02  │   03:35 min  ┃      0.1251  │     95.14 %  ┃\n",
            "┃         179  ┃      0.0780  │     97.79 %  ┃   2.000e-02  │   03:34 min  ┃      0.1346  │     94.73 %  ┃\n",
            "┃         180  ┃      0.0521  │     98.72 %  ┃   4.000e-03  │   03:35 min  ┃      0.0882  │     96.61 %  ┃\n",
            "┃         181  ┃      0.0428  │     98.93 %  ┃   4.000e-03  │   03:35 min  ┃      0.0854  │     96.79 %  ┃\n",
            "┃         182  ┃      0.0383  │     99.09 %  ┃   4.000e-03  │   03:35 min  ┃      0.0830  │     96.98 %  ┃\n",
            "┃         183  ┃      0.0359  │     99.20 %  ┃   4.000e-03  │   03:35 min  ┃      0.0818  │     97.01 %  ┃\n",
            "┃         184  ┃      0.0361  │     99.14 %  ┃   4.000e-03  │   03:35 min  ┃      0.0815  │     96.98 %  ┃\n",
            "┃         185  ┃      0.0348  │     99.21 %  ┃   4.000e-03  │   03:35 min  ┃      0.0810  │     96.94 %  ┃\n",
            "┃         186  ┃      0.0334  │     99.23 %  ┃   4.000e-03  │   03:35 min  ┃      0.0812  │     96.90 %  ┃\n",
            "┃         187  ┃      0.0326  │     99.28 %  ┃   4.000e-03  │   03:35 min  ┃      0.0814  │     96.99 %  ┃\n",
            "┃         188  ┃      0.0330  │     99.25 %  ┃   4.000e-03  │   03:35 min  ┃      0.0811  │     96.87 %  ┃\n",
            "┃         189  ┃      0.0316  │     99.28 %  ┃   4.000e-03  │   03:35 min  ┃      0.0797  │     97.02 %  ┃\n",
            "┃         190  ┃      0.0314  │     99.34 %  ┃   4.000e-03  │   03:35 min  ┃      0.0792  │     96.99 %  ┃\n",
            "┃         191  ┃      0.0301  │     99.37 %  ┃   4.000e-03  │   03:36 min  ┃      0.0794  │     97.02 %  ┃\n",
            "┃         192  ┃      0.0309  │     99.32 %  ┃   4.000e-03  │   03:36 min  ┃      0.0786  │     96.96 %  ┃\n",
            "┃         193  ┃      0.0309  │     99.28 %  ┃   4.000e-03  │   03:35 min  ┃      0.0811  │     96.94 %  ┃\n",
            "┃         194  ┃      0.0306  │     99.30 %  ┃   4.000e-03  │   03:35 min  ┃      0.0797  │     96.95 %  ┃\n",
            "┃         195  ┃      0.0291  │     99.35 %  ┃   4.000e-03  │   03:36 min  ┃      0.0784  │     96.88 %  ┃\n",
            "┃         196  ┃      0.0288  │     99.38 %  ┃   4.000e-03  │   03:35 min  ┃      0.0790  │     96.99 %  ┃\n",
            "┃         197  ┃      0.0294  │     99.31 %  ┃   4.000e-03  │   03:36 min  ┃      0.0805  │     96.76 %  ┃\n",
            "┃         198  ┃      0.0291  │     99.36 %  ┃   4.000e-03  │   03:36 min  ┃      0.0778  │     97.15 %  ┃\n",
            "┃         199  ┃      0.0295  │     99.29 %  ┃   4.000e-03  │   03:36 min  ┃      0.0785  │     97.01 %  ┃\n",
            "┃         200  ┃      0.0282  │     99.39 %  ┃   4.000e-03  │   03:36 min  ┃      0.0801  │     96.93 %  ┃\n",
            "┃         201  ┃      0.0283  │     99.38 %  ┃   4.000e-03  │   03:36 min  ┃      0.0792  │     96.89 %  ┃\n",
            "┃         202  ┃      0.0286  │     99.39 %  ┃   4.000e-03  │   03:35 min  ┃      0.0793  │     96.87 %  ┃\n",
            "┃         203  ┃      0.0282  │     99.41 %  ┃   4.000e-03  │   03:35 min  ┃      0.0794  │     97.03 %  ┃\n",
            "┃         204  ┃      0.0274  │     99.42 %  ┃   4.000e-03  │   03:36 min  ┃      0.0793  │     96.91 %  ┃\n",
            "┃         205  ┃      0.0280  │     99.38 %  ┃   4.000e-03  │   03:36 min  ┃      0.0795  │     96.94 %  ┃\n",
            "┃         206  ┃      0.0278  │     99.38 %  ┃   4.000e-03  │   03:36 min  ┃      0.0789  │     96.91 %  ┃\n",
            "┃         207  ┃      0.0280  │     99.39 %  ┃   4.000e-03  │   03:35 min  ┃      0.0785  │     97.00 %  ┃\n",
            "┃         208  ┃      0.0270  │     99.40 %  ┃   4.000e-03  │   03:36 min  ┃      0.0806  │     96.82 %  ┃\n",
            "┃         209  ┃      0.0281  │     99.36 %  ┃   4.000e-03  │   03:36 min  ┃      0.0801  │     96.87 %  ┃\n",
            "┃         210  ┃      0.0281  │     99.35 %  ┃   4.000e-03  │   03:35 min  ┃      0.0808  │     96.94 %  ┃\n",
            "┃         211  ┃      0.0275  │     99.43 %  ┃   4.000e-03  │   03:35 min  ┃      0.0816  │     96.85 %  ┃\n",
            "┃         212  ┃      0.0281  │     99.40 %  ┃   4.000e-03  │   03:36 min  ┃      0.0800  │     96.78 %  ┃\n",
            "┃         213  ┃      0.0277  │     99.43 %  ┃   4.000e-03  │   03:36 min  ┃      0.0820  │     96.91 %  ┃\n",
            "┃         214  ┃      0.0275  │     99.38 %  ┃   4.000e-03  │   03:35 min  ┃      0.0802  │     96.82 %  ┃\n",
            "┃         215  ┃      0.0277  │     99.39 %  ┃   4.000e-03  │   03:36 min  ┃      0.0800  │     96.89 %  ┃\n",
            "┃         216  ┃      0.0277  │     99.35 %  ┃   4.000e-03  │   03:36 min  ┃      0.0831  │     96.76 %  ┃\n",
            "┃         217  ┃      0.0273  │     99.37 %  ┃   4.000e-03  │   03:36 min  ┃      0.0818  │     96.76 %  ┃\n",
            "┃         218  ┃      0.0274  │     99.41 %  ┃   4.000e-03  │   03:36 min  ┃      0.0789  │     96.97 %  ┃\n",
            "┃         219  ┃      0.0278  │     99.37 %  ┃   4.000e-03  │   03:35 min  ┃      0.0821  │     96.78 %  ┃\n",
            "┃         220  ┃      0.0288  │     99.34 %  ┃   4.000e-03  │   03:36 min  ┃      0.0800  │     96.93 %  ┃\n",
            "┃         221  ┃      0.0270  │     99.44 %  ┃   4.000e-03  │   03:36 min  ┃      0.0840  │     96.73 %  ┃\n",
            "┃         222  ┃      0.0274  │     99.39 %  ┃   4.000e-03  │   03:36 min  ┃      0.0822  │     96.83 %  ┃\n",
            "┃         223  ┃      0.0273  │     99.44 %  ┃   4.000e-03  │   03:36 min  ┃      0.0814  │     96.89 %  ┃\n",
            "┃         224  ┃      0.0280  │     99.35 %  ┃   4.000e-03  │   03:36 min  ┃      0.0796  │     96.94 %  ┃\n",
            "┃         225  ┃      0.0270  │     99.39 %  ┃   4.000e-03  │   03:36 min  ┃      0.0820  │     96.71 %  ┃\n",
            "┃         226  ┃      0.0270  │     99.41 %  ┃   4.000e-03  │   03:36 min  ┃      0.0821  │     96.76 %  ┃\n",
            "┃         227  ┃      0.0273  │     99.40 %  ┃   4.000e-03  │   03:35 min  ┃      0.0806  │     97.00 %  ┃\n",
            "┃         228  ┃      0.0282  │     99.37 %  ┃   4.000e-03  │   03:36 min  ┃      0.0852  │     96.78 %  ┃\n",
            "┃         229  ┃      0.0275  │     99.43 %  ┃   4.000e-03  │   03:36 min  ┃      0.0804  │     96.88 %  ┃\n",
            "┃         230  ┃      0.0279  │     99.39 %  ┃   4.000e-03  │   03:36 min  ┃      0.0819  │     96.94 %  ┃\n",
            "┃         231  ┃      0.0285  │     99.35 %  ┃   4.000e-03  │   03:36 min  ┃      0.0841  │     96.63 %  ┃\n",
            "┃         232  ┃      0.0278  │     99.43 %  ┃   4.000e-03  │   03:35 min  ┃      0.0840  │     96.79 %  ┃\n",
            "┃         233  ┃      0.0270  │     99.45 %  ┃   4.000e-03  │   03:36 min  ┃      0.0832  │     96.66 %  ┃\n",
            "┃         234  ┃      0.0275  │     99.40 %  ┃   4.000e-03  │   03:35 min  ┃      0.0836  │     96.76 %  ┃\n",
            "┃         235  ┃      0.0273  │     99.40 %  ┃   4.000e-03  │   03:36 min  ┃      0.0835  │     96.68 %  ┃\n",
            "┃         236  ┃      0.0272  │     99.42 %  ┃   4.000e-03  │   03:36 min  ┃      0.0833  │     96.85 %  ┃\n",
            "┃         237  ┃      0.0270  │     99.42 %  ┃   4.000e-03  │   03:35 min  ┃      0.0819  │     96.78 %  ┃\n",
            "┃         238  ┃      0.0284  │     99.35 %  ┃   4.000e-03  │   03:36 min  ┃      0.0832  │     96.78 %  ┃\n",
            "┃         239  ┃      0.0281  │     99.41 %  ┃   4.000e-03  │   03:36 min  ┃      0.0831  │     96.73 %  ┃\n",
            "┃         240  ┃      0.0236  │     99.52 %  ┃   8.000e-04  │   03:35 min  ┃      0.0757  │     97.15 %  ┃\n",
            "┃         241  ┃      0.0222  │     99.53 %  ┃   8.000e-04  │   03:36 min  ┃      0.0742  │     97.21 %  ┃\n",
            "┃         242  ┃      0.0212  │     99.59 %  ┃   8.000e-04  │   03:36 min  ┃      0.0744  │     97.15 %  ┃\n",
            "┃         243  ┃      0.0214  │     99.60 %  ┃   8.000e-04  │   03:35 min  ┃      0.0739  │     97.11 %  ┃\n",
            "┃         244  ┃      0.0205  │     99.57 %  ┃   8.000e-04  │   03:36 min  ┃      0.0724  │     97.28 %  ┃\n",
            "┃         245  ┃      0.0198  │     99.60 %  ┃   8.000e-04  │   03:35 min  ┃      0.0739  │     97.17 %  ┃\n",
            "┃         246  ┃      0.0204  │     99.58 %  ┃   8.000e-04  │   03:36 min  ┃      0.0737  │     97.23 %  ┃\n",
            "┃         247  ┃      0.0196  │     99.62 %  ┃   8.000e-04  │   03:36 min  ┃      0.0733  │     97.21 %  ┃\n",
            "┃         248  ┃      0.0209  │     99.57 %  ┃   8.000e-04  │   03:36 min  ┃      0.0737  │     97.26 %  ┃\n",
            "┃         249  ┃      0.0211  │     99.56 %  ┃   8.000e-04  │   03:36 min  ┃      0.0724  │     97.23 %  ┃\n",
            "┃         250  ┃      0.0198  │     99.61 %  ┃   8.000e-04  │   03:36 min  ┃      0.0733  │     97.23 %  ┃\n",
            "┃         251  ┃      0.0197  │     99.58 %  ┃   8.000e-04  │   03:36 min  ┃      0.0729  │     97.29 %  ┃\n",
            "┃         252  ┃      0.0195  │     99.61 %  ┃   8.000e-04  │   03:36 min  ┃      0.0722  │     97.20 %  ┃\n",
            "┃         253  ┃      0.0192  │     99.63 %  ┃   8.000e-04  │   03:35 min  ┃      0.0722  │     97.24 %  ┃\n",
            "┃         254  ┃      0.0190  │     99.62 %  ┃   8.000e-04  │   03:35 min  ┃      0.0730  │     97.24 %  ┃\n",
            "┃         255  ┃      0.0196  │     99.58 %  ┃   8.000e-04  │   03:35 min  ┃      0.0728  │     97.17 %  ┃\n",
            "┃         256  ┃      0.0189  │     99.61 %  ┃   8.000e-04  │   03:36 min  ┃      0.0727  │     97.32 %  ┃\n",
            "┃         257  ┃      0.0195  │     99.59 %  ┃   8.000e-04  │   03:35 min  ┃      0.0731  │     97.30 %  ┃\n",
            "┃         258  ┃      0.0195  │     99.59 %  ┃   8.000e-04  │   03:36 min  ┃      0.0734  │     97.19 %  ┃\n",
            "┃         259  ┃      0.0193  │     99.63 %  ┃   8.000e-04  │   03:36 min  ┃      0.0725  │     97.27 %  ┃\n",
            "┃         260  ┃      0.0191  │     99.60 %  ┃   8.000e-04  │   03:36 min  ┃      0.0728  │     97.25 %  ┃\n",
            "┃         261  ┃      0.0193  │     99.63 %  ┃   8.000e-04  │   03:36 min  ┃      0.0724  │     97.28 %  ┃\n",
            "┃         262  ┃      0.0190  │     99.61 %  ┃   8.000e-04  │   03:35 min  ┃      0.0731  │     97.28 %  ┃\n",
            "┃         263  ┃      0.0186  │     99.63 %  ┃   8.000e-04  │   03:36 min  ┃      0.0722  │     97.49 %  ┃\n",
            "┃         264  ┃      0.0176  │     99.66 %  ┃   8.000e-04  │   03:36 min  ┃      0.0726  │     97.29 %  ┃\n",
            "┃         265  ┃      0.0181  │     99.61 %  ┃   8.000e-04  │   03:36 min  ┃      0.0735  │     97.37 %  ┃\n",
            "┃         266  ┃      0.0190  │     99.62 %  ┃   8.000e-04  │   03:35 min  ┃      0.0732  │     97.20 %  ┃\n",
            "┃         267  ┃      0.0187  │     99.63 %  ┃   8.000e-04  │   03:35 min  ┃      0.0726  │     97.26 %  ┃\n",
            "┃         268  ┃      0.0186  │     99.66 %  ┃   8.000e-04  │   03:35 min  ┃      0.0739  │     97.22 %  ┃\n",
            "┃         269  ┃      0.0179  │     99.67 %  ┃   8.000e-04  │   03:36 min  ┃      0.0727  │     97.35 %  ┃\n",
            "┃         270  ┃      0.0188  │     99.60 %  ┃   8.000e-04  │   03:35 min  ┃      0.0736  │     97.23 %  ┃\n",
            "┃         271  ┃      0.0179  │     99.67 %  ┃   8.000e-04  │   03:36 min  ┃      0.0735  │     97.28 %  ┃\n",
            "┃         272  ┃      0.0182  │     99.63 %  ┃   8.000e-04  │   03:35 min  ┃      0.0733  │     97.26 %  ┃\n",
            "┃         273  ┃      0.0181  │     99.66 %  ┃   8.000e-04  │   03:35 min  ┃      0.0735  │     97.29 %  ┃\n",
            "┃         274  ┃      0.0178  │     99.62 %  ┃   8.000e-04  │   03:35 min  ┃      0.0728  │     97.23 %  ┃\n",
            "┃         275  ┃      0.0184  │     99.61 %  ┃   8.000e-04  │   03:35 min  ┃      0.0734  │     97.24 %  ┃\n",
            "┃         276  ┃      0.0179  │     99.65 %  ┃   8.000e-04  │   03:35 min  ┃      0.0729  │     97.29 %  ┃\n",
            "┃         277  ┃      0.0179  │     99.65 %  ┃   8.000e-04  │   03:35 min  ┃      0.0724  │     97.33 %  ┃\n",
            "┃         278  ┃      0.0175  │     99.63 %  ┃   8.000e-04  │   03:36 min  ┃      0.0727  │     97.25 %  ┃\n",
            "┃         279  ┃      0.0173  │     99.70 %  ┃   8.000e-04  │   03:35 min  ┃      0.0738  │     97.24 %  ┃\n",
            "┃         280  ┃      0.0177  │     99.68 %  ┃   8.000e-04  │   03:36 min  ┃      0.0735  │     97.32 %  ┃\n",
            "┃         281  ┃      0.0188  │     99.60 %  ┃   8.000e-04  │   03:36 min  ┃      0.0735  │     97.30 %  ┃\n",
            "┃         282  ┃      0.0182  │     99.60 %  ┃   8.000e-04  │   03:34 min  ┃      0.0730  │     97.35 %  ┃\n",
            "┃         283  ┃      0.0179  │     99.66 %  ┃   8.000e-04  │   03:35 min  ┃      0.0729  │     97.32 %  ┃\n",
            "┃         284  ┃      0.0177  │     99.64 %  ┃   8.000e-04  │   03:35 min  ┃      0.0730  │     97.41 %  ┃\n",
            "┃         285  ┃      0.0175  │     99.64 %  ┃   8.000e-04  │   03:35 min  ┃      0.0722  │     97.28 %  ┃\n",
            "┃         286  ┃      0.0178  │     99.65 %  ┃   8.000e-04  │   03:36 min  ┃      0.0725  │     97.27 %  ┃\n",
            "┃         287  ┃      0.0175  │     99.65 %  ┃   8.000e-04  │   03:36 min  ┃      0.0730  │     97.16 %  ┃\n",
            "┃         288  ┃      0.0176  │     99.68 %  ┃   8.000e-04  │   03:36 min  ┃      0.0741  │     97.21 %  ┃\n",
            "┃         289  ┃      0.0174  │     99.65 %  ┃   8.000e-04  │   03:35 min  ┃      0.0734  │     97.31 %  ┃\n",
            "┃         290  ┃      0.0172  │     99.70 %  ┃   8.000e-04  │   03:35 min  ┃      0.0731  │     97.29 %  ┃\n",
            "┃         291  ┃      0.0173  │     99.66 %  ┃   8.000e-04  │   03:35 min  ┃      0.0744  │     97.22 %  ┃\n",
            "┃         292  ┃      0.0175  │     99.63 %  ┃   8.000e-04  │   03:36 min  ┃      0.0734  │     97.25 %  ┃\n",
            "┃         293  ┃      0.0173  │     99.70 %  ┃   8.000e-04  │   03:36 min  ┃      0.0722  │     97.38 %  ┃\n",
            "┃         294  ┃      0.0181  │     99.64 %  ┃   8.000e-04  │   03:35 min  ┃      0.0732  │     97.18 %  ┃\n",
            "┃         295  ┃      0.0169  │     99.66 %  ┃   8.000e-04  │   03:36 min  ┃      0.0723  │     97.34 %  ┃\n",
            "┃         296  ┃      0.0174  │     99.65 %  ┃   8.000e-04  │   03:35 min  ┃      0.0730  │     97.30 %  ┃\n",
            "┃         297  ┃      0.0175  │     99.65 %  ┃   8.000e-04  │   03:35 min  ┃      0.0723  │     97.37 %  ┃\n",
            "┃         298  ┃      0.0176  │     99.65 %  ┃   8.000e-04  │   03:35 min  ┃      0.0726  │     97.24 %  ┃\n",
            "┃         299  ┃      0.0170  │     99.68 %  ┃   8.000e-04  │   03:35 min  ┃      0.0719  │     97.32 %  ┃\n",
            "Files already downloaded and verified\n",
            "Test Loss: 0.00\n",
            "Test Accuracy on the 10000 test images: 97.21 %\n",
            "Confusion Matrix\n",
            "       plane  car  bird  cat  deer  dog  frog  horse  ship  truck\n",
            "plane    977    0     6    2     0    1     0      0    13      1\n",
            "car        0  989     0    0     0    0     0      0     0     11\n",
            "bird       6    0   966   10     4    3     7      2     2      0\n",
            "cat        3    1     4  941     5   37     3      1     3      2\n",
            "deer       1    0     6    6   978    5     1      3     0      0\n",
            "dog        2    0     4   35     5  951     0      3     0      0\n",
            "frog       4    0     5    6     0    1   984      0     0      0\n",
            "horse      3    0     0    3     6    4     0    984     0      0\n",
            "ship      12    4     1    3     0    0     0      0   979      1\n",
            "truck      3   16     2    0     0    0     0      0     7    972\n",
            "              Label  f1-score  precision    recall     support\n",
            "0             plane  0.971656    0.97700  0.966370   1011.0000\n",
            "1               car  0.984080    0.98900  0.979208   1010.0000\n",
            "2              bird  0.968907    0.96600  0.971831    994.0000\n",
            "3               cat  0.938185    0.94100  0.935388   1006.0000\n",
            "4              deer  0.978979    0.97800  0.979960    998.0000\n",
            "5               dog  0.950050    0.95100  0.949102   1002.0000\n",
            "6              frog  0.986466    0.98400  0.988945    995.0000\n",
            "7             horse  0.987456    0.98400  0.990937    993.0000\n",
            "8              ship  0.977046    0.97900  0.975100   1004.0000\n",
            "9             truck  0.978359    0.97200  0.984802    987.0000\n",
            "accuracy             0.972100    0.97210  0.972100      0.9721\n",
            "macro avg            0.972118    0.97210  0.972164  10000.0000\n",
            "weighted avg         0.972082    0.97209  0.972100  10000.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure of the program I wrote in this CIFAR-10 classified coding assignment can be mainly summarized as WideResNet + SAM + Cutout. Since I'm not very comfortable programming in Jupyter Notebooks, I rewrote the program in my local IDE. The project file consists of three main files called main.py, model.py, and utils.py. The project runs the main program within main.py, the model (WideResNet) are in the model file, and the utils file contains all other functions needed for model training (such as StepLR, SAM optimizer, and Cutout Func for data augmentation). To improve readability, I removed parts of the code that are not important and only show the key results.\n",
        "\n",
        "\n",
        "As for my model's specific implementation, Firstly, I used the WideResNet (WRN-16-8), an enhanced deep residual network architecture which increases channels (or, widths) within the fundamental residual blocks for superior learning capacity. In my experiments,WRN-16-8 got similar results to WRN-28-10, but WRN-28-10 required almost twice as many parameters to train. In addition, I also tried PyramidNet, which is a pyramid-like network structure. Theoretically, the PyramidNet-272 can achieve higher accuracy than WRN-28-10, but I gave up due to the huge model training time; During my trials, PyramidNet-110 was about 0.8 percentage point less accurate than WRN-28-10. Reference: https://arxiv.org/pdf/2011.14660v4.pdf\n",
        "\n",
        "\n",
        "Secondly, the SAM algorithm was employed as the optimizer. SAM simultaneously minimizes loss value and loss sharpness, it needs two forward-backward passes to estimate the \"sharpness-aware\" gradient. Benchmarking the SGD Optimizer, the SAM Optimizer enhances the model's precision by about 0.5 to 1 percentage points within the CIFAR-10 data set. Reference: https://arxiv.org/pdf/2010.01412v3.pdf\n",
        "\n",
        "\n",
        "Lastly, the implementation of the Cutout technique was applied for data augmentation. In my experiments, the model without data augmentation was about 2 percentage points less accurate than the model with data augmentation. Furthermore, the model's learning rate was tweaked via the StepLR Function, which diminishes the learning rate according to an increase in iterations.\n",
        "\n",
        "\n",
        "Cumulatively speaking, this structure yielded a testing accuracy exceeding 97.21%. Theoretically, the combination of this model and optimization algorithm can achieve an accuracy of over 98%, but this requires a longer number of iterations and data augmentation tricks, which is also a big burden on the GPU. The highest accuracy I know of that can be achieved today on the CIFAR-10 dataset is over 99%, but that is achieved with a Vision Transformer (ViT) and requires extra training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "KzlUI0F6Wbzy"
      }
    }
  ]
}